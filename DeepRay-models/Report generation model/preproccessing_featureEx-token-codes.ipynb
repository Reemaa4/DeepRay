{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-12T13:10:48.452412Z",
     "iopub.status.busy": "2025-01-12T13:10:48.452073Z",
     "iopub.status.idle": "2025-01-12T13:10:59.595215Z",
     "shell.execute_reply": "2025-01-12T13:10:59.594068Z",
     "shell.execute_reply.started": "2025-01-12T13:10:48.452383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import os  \n",
    "from PIL import Image  \n",
    "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains chest X-ray reports and associated metadata from Indiana University. It includes two key files:\n",
    "\n",
    "indiana_reports.csv: Contains patient-level information with fields like:\n",
    "uid: Unique identifier for each patient or study.\n",
    "findings: Detailed descriptions of the X-ray observations.\n",
    "impression: A summarized conclusion based on the findings.\n",
    "indication: Clinical reason for performing the X-ray.\n",
    "comparison: Notes on any prior studies for comparison.\n",
    "Additional fields such as MeSH and Problems for medical annotations.\n",
    "indiana_projections.csv: Lists X-ray image file names and their projections (e.g., frontal, lateral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:52.854241Z",
     "iopub.status.busy": "2025-01-05T20:00:52.85367Z",
     "iopub.status.idle": "2025-01-05T20:00:52.988986Z",
     "shell.execute_reply": "2025-01-05T20:00:52.988208Z",
     "shell.execute_reply.started": "2025-01-05T20:00:52.854209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset \n",
    "data = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')\n",
    "\n",
    "# Display summary\n",
    "summary = pd.DataFrame({\n",
    "    'Column': data.columns,\n",
    "    'Non-Null Count': data.notnull().sum().values,\n",
    "    'Data Type': data.dtypes.values,\n",
    "    'Unique Values': [data[col].nunique() for col in data.columns],\n",
    "    'Sample Value': [data[col].dropna().iloc[0] for col in data.columns]\n",
    "})\n",
    "\n",
    "print(\"Dataset Summary Table:\")\n",
    "display(summary)\n",
    "\n",
    "print(\"\\nExample Rows:\")\n",
    "display(data.head(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " the data consist of images,and its assosiated reports.\n",
    " a report could be applied to one or more image because a patient might have several images taken(frontal/Lateral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:52.991019Z",
     "iopub.status.busy": "2025-01-05T20:00:52.990811Z",
     "iopub.status.idle": "2025-01-05T20:00:53.298399Z",
     "shell.execute_reply": "2025-01-05T20:00:53.297572Z",
     "shell.execute_reply.started": "2025-01-05T20:00:52.991002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\n",
    "\n",
    "# Count the number of images associated with each report\n",
    "image_counts = data.groupby('uid')['filename'].count()\n",
    "\n",
    "# Count how many reports are associated with each image count\n",
    "image_count_distribution = image_counts.value_counts().sort_index()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(image_count_distribution.index, image_count_distribution.values, width=0.6, edgecolor='black')\n",
    "plt.xlabel('Number of Images per Report', fontsize=14)\n",
    "plt.ylabel('Number of Reports', fontsize=14)\n",
    "plt.title('Distribution of Images Associated with Each Report', fontsize=16)\n",
    "plt.xticks(image_count_distribution.index, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "The dataset consists of chest X-ray images and corresponding radiology reports . Each report is linked to one or more images a, including projections like frontal and lateral views,and as we see in the graph,most reports connected with two images and less with 1,3 or 4. The goal is to associate each report with exactly two images:\n",
    "- If there is only one image, it will be duplicated.\n",
    "- If there are more than two images, we prioritize selecting one frontal and one lateral view. If both are not available, the first two images will be selected.\n",
    "\n",
    "## Steps\n",
    "1. **Merge Data**: Combine the `indiana_projections.csv` and `indiana_reports.csv` files using the `uid` column.\n",
    "2. **Image Selection**:\n",
    "   - For each report (`uid`), determine the available projections.\n",
    "   - Select one frontal and one lateral view, if available.\n",
    "   - If only one image exists, duplicate it.\n",
    "   - If more than two images exist but only one type of projection, pick the first two images.\n",
    "3. **Save Processed Dataset**: The resulting dataset is saved as `processed_dataset.csv`.\n",
    "\n",
    "## Code\n",
    "The following code implements the above logic:\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:53.299976Z",
     "iopub.status.busy": "2025-01-05T20:00:53.299701Z",
     "iopub.status.idle": "2025-01-05T20:00:56.433581Z",
     "shell.execute_reply": "2025-01-05T20:00:56.432808Z",
     "shell.execute_reply.started": "2025-01-05T20:00:53.299952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the original datasets\n",
    "projections_df = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\n",
    "reports_df = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')\n",
    "\n",
    "# Merge the datasets on 'uid'\n",
    "data = pd.merge(projections_df, reports_df, on='uid')\n",
    "\n",
    "# Functio to select exactly two images per report and format the output\n",
    "def select_images_for_csv(group):\n",
    "    frontal = group[group['projection'] == 'Frontal']\n",
    "    lateral = group[group['projection'] == 'Lateral']\n",
    "\n",
    "    if len(group) == 1:  # Only one image, duplicate it\n",
    "        return {\n",
    "            \"Person_id\": group.iloc[0][\"uid\"],\n",
    "            \"Image1\": group.iloc[0][\"filename\"],\n",
    "            \"Image2\": group.iloc[0][\"filename\"],\n",
    "            \"Report\": group.iloc[0][\"findings\"]\n",
    "        }\n",
    "\n",
    "    if len(frontal) > 0 and len(lateral) > 0:  # At least one frontal and one lateral\n",
    "        return {\n",
    "            \"Person_id\": group.iloc[0][\"uid\"],\n",
    "            \"Image1\": frontal.iloc[0][\"filename\"],\n",
    "            \"Image2\": lateral.iloc[0][\"filename\"],\n",
    "            \"Report\": group.iloc[0][\"findings\"]\n",
    "        }\n",
    "\n",
    "    # If only one type of projection exists or more than two images, pick the first two\n",
    "    selected_images = group.iloc[:2]\n",
    "    return {\n",
    "        \"Person_id\": group.iloc[0][\"uid\"],\n",
    "        \"Image1\": selected_images.iloc[0][\"filename\"],\n",
    "        \"Image2\": selected_images.iloc[1][\"filename\"],\n",
    "        \"Report\": group.iloc[0][\"findings\"]\n",
    "    }\n",
    "\n",
    "# Apply the selection function and create the desired CSV\n",
    "output_data = data.groupby('uid').apply(select_images_for_csv).tolist()\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the new CSV file\n",
    "output_df.to_csv('formatted_dataset.csv', index=False)\n",
    "\n",
    "print(\"Formatted dataset saved as 'formatted_dataset.csv'.\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "formatted_data = pd.read_csv('formatted_dataset.csv')\n",
    "\n",
    "# Display the first 10 rows of the dataset as a visually pleasing table\n",
    "display(formatted_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove all numbers, stop words from our text data. Also we will convert all the text in lower case and perform deconstruction on the each report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:56.434633Z",
     "iopub.status.busy": "2025-01-05T20:00:56.43435Z",
     "iopub.status.idle": "2025-01-05T20:00:57.999702Z",
     "shell.execute_reply": "2025-01-05T20:00:57.998961Z",
     "shell.execute_reply.started": "2025-01-05T20:00:56.434612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define preprocessing functions\n",
    "def lowercase(text):\n",
    "    '''Converts to lowercase'''\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        if isinstance(line, str):  # Check if the line is a string\n",
    "            new_text.append(line.lower())\n",
    "        else:\n",
    "            new_text.append(\"\")  # Replace non-string values with an empty string\n",
    "    return new_text\n",
    "\n",
    "def decontractions(text):\n",
    "    '''Performs decontractions in the doc'''\n",
    "    new_text = []\n",
    "    for phrase in text:\n",
    "        if isinstance(phrase, str):  # Check if the phrase is a string\n",
    "            phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "            phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "            phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "            phrase = re.sub(r\"shouldn\\'t\", \"should not\", phrase)\n",
    "            phrase = re.sub(r\"wouldn\\'t\", \"would not\", phrase)\n",
    "            # general\n",
    "            phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "            phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "            phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "            phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "            phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "            phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "            phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        new_text.append(phrase)\n",
    "    return new_text\n",
    "\n",
    "def rem_punctuations(text):\n",
    "    '''Removes punctuations'''\n",
    "    punctuations = '''!()-[]{};:'\"\\\\,<>/?@#$%^&*~'''\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        if isinstance(line, str):  # Check if the line is a string\n",
    "            for char in line:\n",
    "                if char in punctuations:\n",
    "                    line = line.replace(char, \"\")\n",
    "            new_text.append(' '.join(e for e in line.split()))\n",
    "        else:\n",
    "            new_text.append(\"\")\n",
    "    return new_text\n",
    "\n",
    "def rem_numbers(text):\n",
    "    '''Removes numbers and irrelevant text like xxxx*'''\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        if isinstance(line, str):  # Check if the line is a string\n",
    "            temp = re.sub(r'x*', '', line)\n",
    "            new_text.append(re.sub(r'\\d', '', temp))\n",
    "        else:\n",
    "            new_text.append(\"\")\n",
    "    return new_text\n",
    "\n",
    "def rem_stopwords(text):\n",
    "    '''Removes stop words from the text but preserves negations.'''\n",
    "    negations = {'no', 'not'}\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        if isinstance(line, str):  # Check if the line is a string\n",
    "            temp = line.split()\n",
    "            temp2 = [word for word in temp if word not in stop_words or word in negations]\n",
    "            new_text.append(' '.join(temp2))\n",
    "        else:\n",
    "            new_text.append(\"\")\n",
    "    return new_text\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    '''Combines all the preprocess functions'''\n",
    "    new_text = lowercase(text)\n",
    "    new_text = decontractions(new_text)\n",
    "    new_text = rem_punctuations(new_text)\n",
    "    new_text = rem_numbers(new_text)\n",
    "    new_text = rem_stopwords(new_text)\n",
    "    return new_text\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/working/formatted_dataset.csv')\n",
    "\n",
    "# Fill NaN values in 'Report' column with empty strings\n",
    "df['Report'] = df['Report'].fillna(\"\")\n",
    "\n",
    "# Preprocess the 'Report' column\n",
    "df['Report'] = text_preprocessing(df['Report'])\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "df.to_csv('preprocessed_dataset2.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing complete. Dataset saved as 'preprocessed_dataset2.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we will remove rows with empty report field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:58.000826Z",
     "iopub.status.busy": "2025-01-05T20:00:58.000515Z",
     "iopub.status.idle": "2025-01-05T20:00:58.041813Z",
     "shell.execute_reply": "2025-01-05T20:00:58.040936Z",
     "shell.execute_reply.started": "2025-01-05T20:00:58.000796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/kaggle/working/preprocessed_dataset2.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Count the initial number of rows\n",
    "initial_count = len(df)\n",
    "\n",
    "# Remove rows with empty 'Report' fields\n",
    "df_cleaned = df[df['Report'].notnull() & (df['Report'].str.strip() != '')]\n",
    "\n",
    "# Count the final number of rows\n",
    "final_count = len(df_cleaned)\n",
    "\n",
    "# Calculate the number of rows removed\n",
    "rows_removed = initial_count - final_count\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows removed: {rows_removed}\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_cleaned.to_csv('cleaned_preprocessed_dataset2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for NLP tasks,we must add 'startseq' and 'endseq' tokens to each sentence, to prepare the Report data for sequence modeling tasks,it helps the model know where a sentence starts and ends, which is critical for tasks like text generation or captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:58.042965Z",
     "iopub.status.busy": "2025-01-05T20:00:58.042683Z",
     "iopub.status.idle": "2025-01-05T20:00:58.084024Z",
     "shell.execute_reply": "2025-01-05T20:00:58.083263Z",
     "shell.execute_reply.started": "2025-01-05T20:00:58.042945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/kaggle/working/cleaned_preprocessed_dataset2.csv'  # Path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the remodelling function\n",
    "def remodelling(x): \n",
    "    '''Adds start and end tokens to a sentence'''\n",
    "    return 'startseq ' + str(x) + ' endseq'\n",
    "\n",
    "# Apply the remodelling function to the 'Report' column\n",
    "df['Report'] = df['Report'].apply(lambda x: remodelling(x))\n",
    "\n",
    "# Save the modified dataset\n",
    "df.to_csv('/kaggle/working/Ready_dataset2.csv', index=False)\n",
    "\n",
    "print(\"Start and end tokens added. Modified dataset saved as '/kaggle/working/Ready_dataset2.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will split our data into training,testing and cross validation (cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T20:00:58.086574Z",
     "iopub.status.busy": "2025-01-05T20:00:58.086289Z",
     "iopub.status.idle": "2025-01-05T20:00:58.132977Z",
     "shell.execute_reply": "2025-01-05T20:00:58.132324Z",
     "shell.execute_reply.started": "2025-01-05T20:00:58.086552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/working/Ready_dataset2.csv')\n",
    "\n",
    "# Split dataset\n",
    "train, temp = train_test_split(df, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)  # 15% val, 15% test\n",
    "\n",
    "# Save the splits \n",
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have done all the preprocessing needed and we have our final split of training,testing and cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will  make sure that our images are in a good shape to feed them to the model for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an Encoder-Decoder architecture:\n",
    "Encoder: Use a CNN (CheXNet) which produces a context vector by taking in our image features.\n",
    "Decoder: Use an RNN ( LSTM or GRU) .\n",
    "and use beam search to generate the reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Image Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and reports are the input to the model we will convert every image into fixed size vector to be fed to the model, we will use transfer learning for that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using pre trained cheXnet model to extract features from images, cheXnet is a 121-layer convolutional neural network trained on chest xray to classify 14 diseases however our purpose is not to classify the images but to get the features for each image so we will discard the last classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import densenet\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets \n",
    "train_data = pd.read_csv('/kaggle/input/chstx-extracted-features-and-other-files/train-test-cv_split/train.csv')\n",
    "val_data = pd.read_csv('/kaggle/input/chstx-extracted-features-and-other-files/train-test-cv_split/val.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/chstx-extracted-features-and-other-files/train-test-cv_split/test.csv')\n",
    "\n",
    "# Check the data\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    # Prepend the base path to the given image path\n",
    "    full_path = BASE_PATH + img_path\n",
    "    image = Image.open(full_path)\n",
    "    img_array = np.asarray(image.convert(\"RGB\"))\n",
    "    img_array = resize(img_array, (224, 224, 3))  # Resize to DenseNet input size\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return densenet.preprocess_input(img_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download cheXnet weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have already downloaded the weights and saved it in the data set we created(to avoid repeating runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained CheXNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added an average pooling layer at the end to avoid the model learning the order of the images. Without pooling, the model might think that image1 always comes before image2, which isn’t true. The pooling makes the model ignore the positions of features and focus only on their overall patterns, making the feature extraction better and more flexible.(in the feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load DenseNet model without the final classification layer\n",
    "chexNet = densenet.DenseNet121(include_top=False, weights=None, input_shape=(224, 224, 3), pooling=\"avg\")\n",
    "\n",
    "# Add custom output layer for feature extraction\n",
    "X = chexNet.output\n",
    "X = tf.keras.layers.Dense(14, activation=\"sigmoid\", name=\"predictions\")(X)\n",
    "model = tf.keras.Model(inputs=chexNet.input, outputs=X)\n",
    "\n",
    "# Load pre-trained weights for CheXNet\n",
    "model.load_weights('/kaggle/input/chstx-extracted-features-and-other-files/CheXNet_Keras_weights.h5')\n",
    "\n",
    "# Feature extractor model\n",
    "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "feature_extractor.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no need to run it each time(the features were saved and uploaded as input for faster run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  \n",
    "\n",
    "# Feature extraction process\n",
    "def extract_features(data):\n",
    "    features = {}\n",
    "    first_key = None\n",
    "    for i, (key, img1, img2, report) in enumerate(\n",
    "        tqdm(data.itertuples(index=False), total=len(data), desc=\"Processing Images\")\n",
    "    ):\n",
    "        # Load and process both images\n",
    "        img1_features = feature_extractor.predict(load_image(img1), verbose=0)\n",
    "        img2_features = feature_extractor.predict(load_image(img2), verbose=0)\n",
    "\n",
    "        # Combine the features from both images\n",
    "        combined_features = np.concatenate((img1_features, img2_features), axis=1)\n",
    "\n",
    "        # Save combined features using the unique key\n",
    "        features[key] = combined_features\n",
    "\n",
    "        # Print the shape of the first feature vector only\n",
    "        if i == 0:\n",
    "            first_key = key\n",
    "            print(f\"First Feature Vector Shape for Key '{first_key}': {combined_features.shape}\")\n",
    "\n",
    "    return features\n",
    "\n",
    "# Dictionary to store all features\n",
    "all_features = {}\n",
    "\n",
    "# Extract features for the entire dataset\n",
    "print(\"Extracting features for the training set...\")\n",
    "all_features['train'] = extract_features(train_data)\n",
    "\n",
    "print(\"Extracting features for the validation set...\")\n",
    "all_features['val'] = extract_features(val_data)\n",
    "\n",
    "print(\"Extracting features for the test set...\")\n",
    "all_features['test'] = extract_features(test_data)\n",
    "\n",
    "# Save all extracted features into a single file\n",
    "print(\"Saving all extracted features into one file...\")\n",
    "with open('all_features2.pickle', 'wb') as f:\n",
    "    pickle.dump(all_features, f)\n",
    "\n",
    "print(\"Feature extraction and saving completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verification for quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "features_file = '/kaggle/input/chstx-extracted-features-and-other-files/all_features2.pickle'  \n",
    "train_csv = '/kaggle/input/chstx-extracted-features-and-other-files/train-test-cv_split/train.csv'  \n",
    "val_csv = '/kaggle/input/chstx-extracted-features-and-other-files/train-test-cv_split/val.csv'     \n",
    "test_csv = '/kaggle/input/chstx-extracted-features-and-other-files/train-test-cv_split/test.csv'   \n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(train_csv)\n",
    "val_data = pd.read_csv(val_csv)\n",
    "test_data = pd.read_csv(test_csv)\n",
    "\n",
    "# Load the extracted features\n",
    "with open(features_file, 'rb') as f:\n",
    "    all_features = pickle.load(f)\n",
    "\n",
    "# Quality checks\n",
    "def check_features():\n",
    "    results = {}\n",
    "    for dataset_name, data, csv_data in zip(\n",
    "        ['train', 'val', 'test'], \n",
    "        [all_features['train'], all_features['val'], all_features['test']], \n",
    "        [train_data, val_data, test_data]\n",
    "    ):\n",
    "        print(f\"\\n--- Checking {dataset_name} dataset ---\")\n",
    "\n",
    "        # Check the number of keys\n",
    "        num_keys = len(data)\n",
    "        expected_samples = len(csv_data)\n",
    "        print(f\"Number of keys: {num_keys}, Expected: {expected_samples}\")\n",
    "        if num_keys != expected_samples:\n",
    "            print(f\"WARNING: Mismatch in number of keys for {dataset_name}!\")\n",
    "\n",
    "        # Check the feature vector shape\n",
    "        if num_keys > 0:\n",
    "            key_to_check = list(data.keys())[0]\n",
    "            feature_shape = data[key_to_check].shape\n",
    "            print(f\"Feature vector shape for key '{key_to_check}': {feature_shape}\")\n",
    "            if feature_shape != (1, 2048):\n",
    "                print(f\"WARNING: Feature vector shape is incorrect for {dataset_name}!\")\n",
    "\n",
    "            # Check feature vector values\n",
    "            feature_values = data[key_to_check]\n",
    "            if np.all(feature_values == 0):\n",
    "                print(f\"WARNING: All feature values are zero for key '{key_to_check}'!\")\n",
    "            elif np.isnan(feature_values).any():\n",
    "                print(f\"WARNING: Feature values contain NaN for key '{key_to_check}'!\")\n",
    "            else:\n",
    "                print(f\"Feature values look normal for key '{key_to_check}'.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"WARNING: {dataset_name} dataset is empty!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run checks\n",
    "check_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we did preprocessing on the text we need to convert it into victors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segregate the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step splits the data into:\n",
    "\n",
    "X:  image feature IDs[image referance].\n",
    "\n",
    "y: Medical reports (text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Segregate the dataset into inputs (features) and outputs (reports)\n",
    "X_train = train_data['Person_id']\n",
    "X_test = test_data['Person_id']\n",
    "X_val = val_data['Person_id']\n",
    "\n",
    "y_train = train_data['Report']\n",
    "y_test = test_data['Report']\n",
    "y_val = val_data['Report']\n",
    "\n",
    "print(f\"Training Set - X: {len(X_train)}, y: {len(y_train)}\")\n",
    "print(f\"Validation Set - X: {len(X_val)}, y: {len(y_val)}\")\n",
    "print(f\"Test Set - X: {len(X_test)}, y: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts text reports into numeric tokens.\n",
    "Vocabulary size: Total unique words in y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit_on_texts method reads all the text data and creates a vocabulary of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize and fit the tokenizer on the training set reports\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(y_train.values)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Example Tokenized Report: {tokenizer.texts_to_sequences([y_train.values[0]])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Padding Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets a maximum length for all sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define maximum sequence length for padding\n",
    "padding_size = 153  # Maximum report length\n",
    "vocab_size = len(tokenizer.word_index.keys()) + 1  # Add 1 for padding token\n",
    "\n",
    "print(f\"Padding Size: {padding_size}, Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pre-trained GloVe Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads pre-trained GloVe embeddings (e.g., 300-dimensional vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load GloVe vectors\n",
    "with open('/kaggle/input/chstx-extracted-features-and-other-files/glove_vectors300d.pickle', 'rb') as f:  \n",
    "    glove_vectors = pickle.load(f)\n",
    "\n",
    "print(f\"Sample GloVe Vector for 'lungs': {glove_vectors.get('lungs', 'Not found')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Embedding Matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matrix: Maps tokens to their GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 300))  # 300 dimensions from GloVe\n",
    "\n",
    "# Populate embedding matrix with GloVe vectors\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[i] = glove_vectors[word]\n",
    "\n",
    "print(f\"Shape of Embedding Matrix: {embedding_matrix.shape}\")\n",
    "print(f\"Sample Embedding Vector: {embedding_matrix[tokenizer.word_index['lungs']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save (Tokenizer, Padding Size, Vocabulary Size, and Embedding Matrix)for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Save Tokenizer, Padding Size, Vocab Size, and Embedding Matrix together\n",
    "save_data = {\n",
    "    'tokenizer': tokenizer,\n",
    "    'padding_size': padding_size,\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/tokenizer_embedding_config.pickle', 'wb') as handle:\n",
    "    pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Tokenizer, padding size, vocab size, and embedding matrix saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the extracted features from the pickle file\n",
    "with open('/kaggle/input/chstx-extracted-features-and-other-files/all_features2.pickle', 'rb') as f:\n",
    "    cheXnet_Features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(id_, report, dataset_name=\"train\"):\n",
    "    \"\"\"\n",
    "    Loads the Image Features with their corresponding IDs from cheXnet_Features.\n",
    "    \"\"\"\n",
    "    id_int = int(id_)  # Ensure ID is an integer\n",
    "    img_feature = cheXnet_Features[dataset_name].get(id_int, None)  # Use dataset_name to access the correct subset\n",
    "    if img_feature is None:\n",
    "        print(f\"Warning: No feature found for ID: {id_int} in {dataset_name} dataset. Skipping...\")\n",
    "        return None, None  # Return None for missing features\n",
    "    return img_feature[0], report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification(locate possible errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to test load_image exhaustively\n",
    "def test_load_image():\n",
    "    # Sample multiple IDs from train_data\n",
    "    print(\"\\n--- Verifying load_image Function ---\")\n",
    "    for idx in range(5):  # Testing first 5 IDs for brevity\n",
    "        sample_id = str(train_data['Person_id'].iloc[idx])  # Ensure the ID is treated as a string\n",
    "        sample_report = train_data['Report'].iloc[idx]\n",
    "        print(f\"\\nTesting ID: {sample_id}, Report: {sample_report[:50]}...\")\n",
    "\n",
    "        # Try loading the feature\n",
    "        try:\n",
    "            sample_feature, sample_report_returned = load_image(sample_id, sample_report)\n",
    "            print(f\"Feature Shape: {sample_feature.shape}\")\n",
    "            print(f\"Report Match: {sample_report == sample_report_returned}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    # Test for a missing ID\n",
    "    print(\"\\nTesting for Missing ID...\")\n",
    "    try:\n",
    "        invalid_id = \"9999999\"  # Non-existent ID\n",
    "        load_image(invalid_id, \"Test Report\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Handled Missing ID Correctly: {e}\")\n",
    "\n",
    "    # Test for an unexpected ID type\n",
    "    print(\"\\nTesting for Unexpected ID Type...\")\n",
    "    try:\n",
    "        invalid_type_id = 123.456  # Float instead of string/int\n",
    "        load_image(invalid_type_id, \"Test Report\")\n",
    "    except Exception as e:\n",
    "        print(f\"Handled Unexpected ID Type: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_load_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dataset_generator(img_name, caption, dataset_name):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset generator that pairs image features with corresponding captions.\n",
    "    \"\"\"\n",
    "\n",
    "    def map_function(item1, item2):\n",
    "        # Load features and filter out None values\n",
    "        img_feature, report = load_image(item1, item2, dataset_name)\n",
    "        if img_feature is None:  # Skip missing features\n",
    "            return tf.constant([], dtype=tf.float32), tf.constant(\"\", dtype=tf.string)\n",
    "        return img_feature, report\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name, caption))\n",
    "\n",
    "    # Use map to load the numpy files in parallel\n",
    "    dataset = dataset.map(\n",
    "        lambda item1, item2: tf.numpy_function(map_function, [item1, item2], [tf.float32, tf.string]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # Filter out invalid (empty) entries\n",
    "    dataset = dataset.filter(lambda x, y: tf.not_equal(tf.size(x), 0))\n",
    "\n",
    "    # Shuffle and batch the dataset\n",
    "    dataset = dataset.shuffle(500).batch(2).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#batch was 12 but nitb changed it to 2 to chk if it is the problem of slowness\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verify_generator(generator, dataset_name, num_samples=3):\n",
    "    \"\"\"\n",
    "    Verifies the dataset generator by printing feature shapes and corresponding reports.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Verifying {dataset_name} Generator ---\")\n",
    "    for batch_num, (features, reports) in enumerate(generator.take(num_samples)):\n",
    "        print(f\"Batch {batch_num + 1}:\")\n",
    "        print(f\" - Feature Shape: {features.shape}\")\n",
    "        print(f\" - Sample Report: {reports.numpy()[0].decode('utf-8')}\")\n",
    "    print(f\"{dataset_name} generator verification completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Re-generate datasets\n",
    "train_generator = dataset_generator(train_data['Person_id'].values, train_data['Report'].values, \"train\")\n",
    "val_generator = dataset_generator(val_data['Person_id'].values, val_data['Report'].values, \"val\")\n",
    "test_generator = dataset_generator(test_data['Person_id'].values, test_data['Report'].values, \"test\")\n",
    "\n",
    "# Verify each generator\n",
    "verify_generator(train_generator, \"train\")\n",
    "verify_generator(val_generator, \"validation\")\n",
    "verify_generator(test_generator, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra verification(irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to check tokenization consistency\n",
    "def verify_tokenizer(tokenizer, sample_texts, num_samples=10):\n",
    "    print(\"\\n--- Tokenizer Verification ---\")\n",
    "    \n",
    "    # Show a few words and their corresponding tokens\n",
    "    for text in sample_texts[:num_samples]:\n",
    "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "        print(f\"\\nOriginal Text: {text}\")\n",
    "        print(f\"Tokenized Sequence: {tokens}\")\n",
    "        \n",
    "        # Reverse mapping: tokens back to words\n",
    "        reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "        reconstructed_text = ' '.join([reverse_word_index.get(token, \"?\") for token in tokens])\n",
    "        print(f\"Reconstructed Text: {reconstructed_text}\")\n",
    "\n",
    "    # Random check for specific words\n",
    "    words_to_check = ['heart', 'lungs', 'effusion', 'normal']\n",
    "    print(\"\\nSpecific Word Token Mapping:\")\n",
    "    for word in words_to_check:\n",
    "        token = tokenizer.word_index.get(word, None)\n",
    "        if token:\n",
    "            print(f\"'{word}' → Token ID: {token}\")\n",
    "        else:\n",
    "            print(f\"Word '{word}' not found in tokenizer vocabulary!\")\n",
    "\n",
    "# Sample text data for verification\n",
    "sample_reports = y_train.sample(5).values  # Randomly sample 5 reports from the training set\n",
    "\n",
    "# Run verification\n",
    "verify_tokenizer(tokenizer, sample_reports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bytes to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts byte-encoded report text data (output by the dataset generator) back into standard string format.  \n",
    "TensorFlow's `tf.numpy_function` outputs text data in bytes, which must be decoded to strings for tokenization and model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bytes_to_string(arr):\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i].decode('utf-8')  # Decode bytes to string\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert(images, reports):\n",
    "    \"\"\"\n",
    "    Converts batches of images and reports into training-ready format:\n",
    "    - Images remain as features.\n",
    "    - Reports are split into input and output sequences for the model.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    in_reports = []\n",
    "    out_reports = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        # Convert report text into a sequence of tokens\n",
    "        sequence = [tokenizer.word_index.get(word, 0) for word in reports[i].split()]\n",
    "        \n",
    "        # Generate input-output pairs for the model\n",
    "        for j in range(1, len(sequence)):\n",
    "            in_seq = sequence[:j]  # Input: partial sequence\n",
    "            out_seq = sequence[j]  # Output: next word\n",
    "            \n",
    "            # One-hot encode the output word\n",
    "            out_seq = tf.keras.utils.to_categorical(out_seq, num_classes=vocab_size)\n",
    "            \n",
    "            # Append to lists\n",
    "            imgs.append(images[i])           # Corresponding image feature\n",
    "            in_reports.append(in_seq)        # Input text sequence\n",
    "            out_reports.append(out_seq)      # One-hot encoded next word\n",
    "\n",
    "    # Pad input sequences to have uniform length\n",
    "    in_reports_padded = tf.keras.preprocessing.sequence.pad_sequences(in_reports, maxlen=padding_size, padding='post')\n",
    "    \n",
    "    return np.array(imgs), np.array(in_reports_padded), np.array(out_reports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Take a sample batch from the train_generator\n",
    "for img_batch, report_batch in train_generator.take(1):\n",
    "    # Decode byte reports back to text\n",
    "    decoded_reports = bytes_to_string(report_batch.numpy())\n",
    "\n",
    "    # Convert the batch into input-output pairs for training\n",
    "    imgs, in_reports, out_reports = convert(img_batch.numpy(), decoded_reports)\n",
    "\n",
    "    # Verification Outputs\n",
    "    print(f\"Image Batch Shape: {imgs.shape}\")\n",
    "    print(f\"Input Reports Shape: {in_reports.shape}\")\n",
    "    print(f\"Output Reports Shape (One-Hot): {out_reports.shape}\")\n",
    "\n",
    "    # Show a sample report with input-output pairs\n",
    "    print(f\"\\nSample Decoded Report:\\n{decoded_reports[0]}\")\n",
    "    print(f\"\\nSample Input Sequence:\\n{in_reports[0]}\")\n",
    "    print(f\"\\nSample Output Word (One-Hot):\\n{out_reports[0]}\")\n",
    "    \n",
    "    # Verify the One-Hot Vector\n",
    "    print(f\"\\nSum of One-Hot Vector (should be 1): {np.sum(out_reports[0])}\")\n",
    "    print(f\"Index of '1' in One-Hot Vector: {np.argmax(out_reports[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have verified everything we will start working on the model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5504,
     "sourceId": 8240,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 516716,
     "sourceId": 951996,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6434870,
     "sourceId": 10531107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
